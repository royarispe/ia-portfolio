---
title: "Backpropagation y Optimizadores: MLPs sobre CIFAR-10"
date:
---

# Backpropagation y Optimizadores: MLPs sobre CIFAR-10

---

## üìù Contexto

En este octavo pr√°ctico nos enfocamos en entender c√≥mo se entrenan realmente las redes neuronales modernas mediante **backpropagation** y distintos **optimizadores**.

A diferencia de los pr√°cticos anteriores, aqu√≠ trabajamos con un dataset real de visi√≥n por computadora: **CIFAR-10**, compuesto por 60.000 im√°genes a color de 32√ó32 p√≠xeles y 10 clases.

El objetivo principal fue construir y entrenar **MLPs (redes multicapa)** a partir de im√°genes aplanadas, explorando:

- arquitecturas con diferente profundidad y n√∫mero de neuronas,
- funciones de activaci√≥n,
- regularizaci√≥n (Dropout, L2, BatchNorm),
- optimizadores como Adam, AdamW y SGD con momentum,
- y el uso de callbacks como EarlyStopping o ReduceLROnPlateau.

Todo el trabajo fue realizado en Google Colab.

## üéØ Objetivos

En este pr√°ctico me propuse:

- Comprender el funcionamiento de **backpropagation** aplicado a redes multicapa.
- Entrenar modelos basados en **MLPs** utilizando im√°genes del dataset CIFAR-10 aplanadas como vectores.
- Comparar distintos tipos de **arquitecturas** (profundidad, ancho, activaciones y regularizaci√≥n).
- Evaluar el impacto de **optimizadores modernos** como Adam, AdamW y SGD con momentum/Nesterov.
- Utilizar **callbacks** para mejorar la estabilidad del entrenamiento (EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard).
- Analizar m√©tricas clave como **accuracy**, **loss**, **matriz de confusi√≥n** y **classification report**.

## üöÄ Desarrollo

### üß† Parte 1 ‚Äì Dataset CIFAR-10 y MLP baseline

El pr√°ctico comenz√≥ trabajando con el dataset **CIFAR-10**, un conjunto de 60.000 im√°genes a color de 32√ó32 p√≠xeles distribuidas en 10 clases.  
Para poder entrenar una red neuronal multicapa (MLP), se realiz√≥ el siguiente preprocesamiento:

1. **Normalizaci√≥n** de las im√°genes al rango `[-1, 1]` para facilitar el entrenamiento.
2. **Split del conjunto de validaci√≥n** tomando el 10% inicial del set de entrenamiento.
3. **Aplanado de cada imagen** (32√ó32√ó3 ‚Üí vector de 3072 valores) para usar capas densas.

Con estos datos se entren√≥ un primer modelo baseline muy sencillo:

- 2 capas densas de **32 neuronas** cada una, con activaci√≥n `ReLU`.
- Capa de salida con **10 neuronas** (softmax).
- Optimizador **Adam**.
- P√©rdida: `sparse_categorical_crossentropy`.
- Entrenado durante 5 √©pocas con batch de 32.

Este modelo logr√≥ alrededor de un **45% de accuracy en test**, lo que sirvi√≥ como punto de referencia para todas las pruebas posteriores.  
El objetivo de este baseline no era obtener alta precisi√≥n, sino contar con un modelo simple para comparar contra arquitecturas m√°s profundas o con distintas t√©cnicas de regularizaci√≥n.

### üß± Parte 2 ‚Äì Arquitecturas modulares con `build_model(...)`

Para poder probar m√∫ltiples configuraciones sin reescribir el modelo desde cero, defin√≠ una funci√≥n modular `build_model(...)` que permite variar:

- n√∫mero de capas y neuronas,
- funci√≥n de activaci√≥n (`relu`, `tanh`, `gelu`),
- uso de **Batch Normalization**,
- aplicaci√≥n de **Dropout**,
- regularizaci√≥n L2,
- y el tipo de inicializador de pesos.

Esta funci√≥n genera un MLP completamente configurable.  
Gracias a ella pude crear modelos tan distintos como:

- Un modelo peque√±o: `hidden_layers=[32, 32]`  
- Un modelo profundo: `hidden_layers=[512, 256, 128]`  
- Modelos con y sin regularizaci√≥n (Dropout, L2)

Construir las arquitecturas de esta forma permiti√≥ comparar el impacto de cada componente sin cambiar el c√≥digo principal del entrenamiento.

### ‚öôÔ∏è Parte 3 ‚Äì Optimizadores con `get_optimizer(...)`

Para evaluar c√≥mo afecta la elecci√≥n del optimizador al proceso de entrenamiento, defin√≠ una funci√≥n llamada `get_optimizer(...)` que permite seleccionar f√°cilmente entre:

- **Adam**
- **AdamW** (Adam con weight decay desacoplado)
- **SGD** con momentum o Nesterov
- **RMSprop**

Al centralizar la elecci√≥n del optimizador, pude probar distintos `learning_rate`, momentos y configuraciones sin modificar el resto del c√≥digo.

Esta separaci√≥n fue clave para comparar:

- la velocidad de convergencia,
- la estabilidad de las curvas de p√©rdida,
- y el comportamiento frente al overfitting.

En general, **Adam** sirvi√≥ como baseline r√°pido y estable, **AdamW** funcion√≥ mejor en modelos grandes con regularizaci√≥n, y **SGD con momentum** mostr√≥ un entrenamiento m√°s lento pero √∫til para entender el efecto de actualizar el gradiente de forma m√°s controlada.

### ‚è±Ô∏è Parte 4 ‚Äì Callbacks y Automatizaci√≥n de Experimentos

Para mejorar la estabilidad del entrenamiento y evitar entrenar de m√°s, incorpor√© un conjunto de **callbacks**:

- **EarlyStopping**, para detener el entrenamiento cuando la m√©trica de validaci√≥n deja de mejorar y restaurar los mejores pesos.
- **ReduceLROnPlateau**, que reduce autom√°ticamente el learning rate cuando la validaci√≥n se estanca.
- **ModelCheckpoint**, que guarda el mejor modelo de cada experimento.
- **TensorBoard**, para visualizar curvas de p√©rdida, accuracy e histogramas.

Luego defin√≠ la funci√≥n `run_experiment(...)`, que automatiza el flujo completo:

1. compila el modelo con el optimizador elegido,  
2. entrena usando los callbacks,  
3. eval√∫a en train, validaci√≥n y test,  
4. guarda m√©tricas, n√∫mero de par√°metros y el propio modelo en una lista global.

Gracias a esta estructura fue posible correr m√∫ltiples configuraciones sin repetir c√≥digo y comparar f√°cilmente los resultados entre experimentos.

### üß™ Parte 5 ‚Äì Experimentos realizados

Con toda la infraestructura lista (arquitecturas modulares, optimizadores y callbacks), ejecut√© varios experimentos para analizar c√≥mo influyen los distintos componentes en el desempe√±o del MLP.

Los tres experimentos principales fueron:

#### 1Ô∏è‚É£ Baseline mejorado
- Arquitectura: `hidden_layers=[32, 32]`
- Activaci√≥n: `ReLU`
- Sin BatchNorm ni Dropout
- Optimizador: **Adam** (`lr=1e-3`)
- Epochs: 10, batch_size: 64

Este modelo sirve como referencia: r√°pido de entrenar, pocos par√°metros y rendimiento moderado.

---

#### 2Ô∏è‚É£ MLP profundo con BatchNorm + Dropout + AdamW
- Arquitectura: `hidden_layers=[512, 256, 128]`
- Activaci√≥n: `ReLU`
- BatchNorm: activo
- Dropout: 0.3
- Regularizaci√≥n L2: `1e-5`
- Inicializaci√≥n: HeNormal
- Optimizador: **AdamW** (`lr=5e-4`, `weight_decay=1e-4`)
- Epochs: 15, batch_size: 128

Este modelo increment√≥ significativamente la capacidad de la red, pero gracias a la combinaci√≥n de BatchNorm, Dropout y L2 logr√≥ una mejor estabilidad y generalizaci√≥n respecto al baseline.

---

#### 3Ô∏è‚É£ Arquitectura grande con `tanh` y SGD+Nesterov
- Arquitectura: `hidden_layers=[512, 512]`
- Activaci√≥n: `tanh`
- Dropout: 0.2
- Regularizaci√≥n L2: `5e-5`
- Optimizador: **SGD** con momentum (0.9) y Nesterov
- Learning rate: `1e-2`
- Epochs: 20, batch_size: 128

El entrenamiento con SGD fue m√°s lento y sensible al learning rate, lo que permiti√≥ observar diferencias claras en la din√°mica de descenso del gradiente comparado con Adam y AdamW.

## üì∏ Evidencias

[Enlace al notebook](https://colab.research.google.com/drive/1HQoXPVoGP2LZgph62k42QS9ALNX5xFwA?usp=sharing)

Para documentar los resultados sin saturar el informe con capturas, inclu√≠ √∫nicamente las visualizaciones m√°s representativas generadas en el notebook:

1. **Curvas de entrenamiento y matriz de confusi√≥n del modelo baseline**  
   Gr√°fico de accuracy y loss por √©poca, √∫til para ver la velocidad de convergencia y el punto donde deja de mejorar.

   ![Curvas](../assets/ut2_p8_1.png){ width="480" }
   ![Matriz](../assets/ut2_p8_1.2.png){ width="480" }

2. **Curvas y matriz de confusi√≥n del modelo profundo con BatchNorm + Dropout + AdamW**  
   Muestran c√≥mo las t√©cnicas de regularizaci√≥n y el optimizador estabilizan el entrenamiento y mejoran la validaci√≥n.

   ![Curvas](../assets/ut2_p8_2.png){ width="480" }
   ![Matriz](../assets/ut2_p8_2.2.png){ width="480" }

3. **Curvas y matriz de confusi√≥n del modelo SGD con momentum / Nesterov**
   Permite identificar f√°cilmente qu√© clases siguen siendo m√°s dif√≠ciles de distinguir.
   
   ![Curvas](../assets/ut2_p8_3.png){ width="480" }
   ![Matriz](../assets/ut2_p8_3.2.png){ width="480" }

4. **Resultados final en formato tabla**

    ![Tabla](../assets/ut2_p8_4.png){ width="480" }

Estas visualizaciones provienen directamente de las funciones:
- `plot_history(...)`
- `plot_confusion_and_report(...)`

y pueden capturarse r√°pidamente desde el Colab al ejecutar cada experimento.

## üí° Reflexi√≥n

Este pr√°ctico fue clave para entender c√≥mo se entrenan realmente las redes neuronales modernas.  
M√°s all√° de construir modelos cada vez m√°s grandes, la parte central del aprendizaje vino de observar c√≥mo cambian las curvas de p√©rdida y accuracy cuando se modifica:

- la arquitectura,
- la regularizaci√≥n,
- el optimizador,
- y el scheduler del learning rate.

Algunas conclusiones importantes:

- **La arquitectura por s√≠ sola no garantiza buen desempe√±o**. Modelos grandes sin BatchNorm o Dropout pueden memorizar r√°pidamente los datos y generalizar mal.
- **Batch Normalization** estabiliza el descenso de gradiente y acelera la convergencia, especialmente en redes profundas.
- **Dropout y L2** resultaron clave para frenar el overfitting cuando el n√∫mero de par√°metros aument√≥.
- **Adam** es un excelente baseline, pero **AdamW** funcion√≥ mejor en modelos con mayor capacidad al controlar expl√≠citamente el weight decay.
- **SGD con momentum y Nesterov** mostr√≥ un comportamiento m√°s lento pero tambi√©n m√°s controlado, lo que ayud√≥ a entender c√≥mo distintos optimizadores ‚Äúrecorren‚Äù la superficie de p√©rdida.
- Los **callbacks** simplificaron enormemente el entrenamiento, evitando wasted epochs y ajustando autom√°ticamente el learning rate.

En resumen, este pr√°ctico no solo profundiz√≥ en la mec√°nica de backpropagation, sino tambi√©n en el criterio necesario para dise√±ar y entrenar redes m√°s grandes de forma eficiente y estable. Fue un paso fundamental para conectar teor√≠a, pr√°ctica y herramientas modernas de Deep Learning.

## üìö Referencias

- [Dataset CIFAR-10 en Keras](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data)
- [Keras Sequential Model](https://www.tensorflow.org/guide/keras/sequential_model)
- [Capa Dense en Keras](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)
- [Activaciones en Keras](https://www.tensorflow.org/api_docs/python/tf/keras/activations)
- [Inicializadores en Keras](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)
- [Regularizaci√≥n L2 (Keras)](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/l2)
- [BatchNormalization (Keras)](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)
- [Dropout (Keras)](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)
- [Optimizador Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)
- [Optimizador AdamW](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/AdamW)
- [Optimizador SGD (Momentum / Nesterov)](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD)
- [Optimizador RMSprop](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop)
- [Callbacks de Keras (EarlyStopping, ReduceLROnPlateau, ModelCheckpoint)](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks)
- [TensorBoard: gu√≠a de uso](https://www.tensorflow.org/tensorboard/get_started)



