---
title: "LLMs con LangChain: Prompting, Plantillas y Salida Estructurada"
date:
---

# LLMs con LangChain: Prompting, Plantillas y Salida Estructurada

---

## ü§® Objetivos de Aprendizaje

En este pr√°ctico trabaj√© con **LLMs integrados en LangChain**, explorando c√≥mo estructurar prompts, controlar par√°metros y obtener salidas robustas para aplicaciones reales. Al finalizar, pude:

- Instanciar modelos de OpenAI mediante `ChatOpenAI` y realizar llamadas b√°sicas.
- Ajustar par√°metros de decodificaci√≥n: `temperature`, `max_tokens`, `top_p`.
- Dise√±ar **prompts reutilizables** con `ChatPromptTemplate` y componerlos con LCEL (`|`).
- Obtener **salidas estructuradas** usando `with_structured_output` (JSON/Pydantic).
- Enviar trazas y m√©tricas a LangSmith para medir tokens, latencia y ejecuci√≥n.
- Comparar enfoques zero-shot vs few-shot y c√≥mo afectan la consistencia del modelo.
- Implementar peque√±as cadenas para traducci√≥n, resumen, Q&A y extracci√≥n de informaci√≥n.

---

## üìã Contexto

Este pr√°ctico se centra en construir la base para un **pipeline profesional de LLMs**:

- Prompts claros y modulares  
- Plantillas reutilizables  
- Salidas predecibles  
- Observabilidad (tokens, latencia, logs)  
- Mini-aplicaciones sin dependencias externas  
- Primer paso hacia RAG y sistemas conversacionales

Antes de pasar a Retrieval, este pr√°ctico permite dominar los conceptos fundamentales del ecosistema LangChain + OpenAI.

---

## üöÄ Desarrollo

### üîß Parte 0 ‚Äî Setup y ‚ÄúHello LLM‚Äù

Para comenzar el pr√°ctico, instal√© las dependencias necesarias de **LangChain**, **LangChain-OpenAI**, **LangSmith**, y utilidades opcionales. Luego configur√© las API keys mediante variables de entorno.

El primer paso fue inicializar un modelo con:

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-5-mini", temperature=0)
resp = llm.invoke("Defin√≠ 'Transformer' en una sola oraci√≥n.")
print(resp.content)
```

‚úî Esto confirm√≥ que la conexi√≥n con el modelo funcionaba correctamente.  
‚úî Adem√°s, `temperature=0` garantiza comportamientos deterministas, ideal para ejercicios evaluables.

---

#### üß† Setup con *Fill-in-the-blanks*

Luego complet√© los par√°metros b√°sicos necesarios:

```python
MODEL = "gpt-5-mini"
TEMP = 0.0

llm = ChatOpenAI(model=MODEL, temperature=TEMP)
print(llm.invoke("Hola! Decime tu versi√≥n en una l√≠nea.").content)
```
Prob√© variaciones cambiando `temperature` para observar diferencias en estilo, creatividad y estabilidad.

#### üìù Observaciones iniciales

- Con **temperature = 0**, las respuestas son m√°s **sobrias**, **directas** y **t√©cnicas**.  
- Con **temperature = 0.7+**, aparecen m√°s **adjetivos**, **met√°foras**, cambios de tono y mayor variabilidad entre ejecuciones.  
- Estos experimentos permitieron ver c√≥mo los par√°metros de decodificaci√≥n afectan incluso a prompts simples.

### üß© Parte 1 ‚Äì Par√°metros de decodificaci√≥n (*temperature*, *max_tokens*, *top_p*)

En esta parte del pr√°ctico experiment√© con los par√°metros que controlan el comportamiento generativo del modelo, especialmente `temperature`, y observ√© c√≥mo afectaban claridad, creatividad y estabilidad de las respuestas.

Primero prob√© con una peque√±a bater√≠a de prompts, por ejemplo:

- `Escrib√≠ un tuit (<=20 palabras) celebrando un paper de IA.`
- `Dame 3 bullets concisos sobre ventajas de los Transformers.`

Para cada uno, ejecut√© el modelo varias veces cambiando `temperature` (0.0, 0.5 y 0.9) y compar√© los resultados.

Tambi√©n complet√© el bloque de *fill-in-the-blanks* configurando:

- `MODEL = "gpt-5-mini"`
- `TEMP` en distintos valores (`0.0`, `0.5`, `0.9`), pidiendo cosas como: *"Escrib√≠ un haiku sobre evaluaci√≥n de modelos."*

Esto me permiti√≥ observar c√≥mo responde el mismo modelo bajo distintos niveles de aleatoriedad.

#### üìù Observaciones de esta parte

- Con **temperature = 0.0** las respuestas son:
  - M√°s sobrias, directas y t√©cnicas.
  - Muy estables entre ejecuciones (casi id√©nticas si repito el mismo prompt).
- Con **temperature ‚âà 0.5**:
  - Se mantiene la coherencia, pero aparecen variaciones ligeras en formulaciones y ejemplos.
  - Buen balance entre precisi√≥n y creatividad.
- Con **temperature ‚â• 0.7**:
  - Surgen m√°s adjetivos, met√°foras y cambios de tono.
  - Mayor variabilidad entre ejecuciones: √∫til para creatividad, menos para tareas evaluables.

Adem√°s:

- `max_tokens` determina cu√°nto puede explayarse el modelo: si es muy bajo tiende a cortar las respuestas.
- `top_p` controla el *nucleus sampling*: al subirlo junto con `temperature` aumenta la diversidad, pero tambi√©n el riesgo de respuestas menos controladas.
- En general, para tareas ‚Äúcerradas‚Äù y evaluables, la combinaci√≥n recomendada es **temperature baja** (cerca de 0) y `top_p` moderado, priorizando claridad y reproducibilidad.

### üß© Parte 2 ‚Äì De texto suelto a plantillas con ChatPromptTemplate + LCEL

En esta secci√≥n trabaj√© con **ChatPromptTemplate**, una herramienta clave para separar instrucciones del contenido y construir prompts reutilizables.

La idea principal es armar una plantilla con estructura clara:

- Mensaje del sistema ‚Üí define el estilo, tono y rol del asistente.
- Mensaje del usuario ‚Üí contiene la variable din√°mica que se completar√° en cada invocaci√≥n.
- Encadenamiento con `|` (LCEL) ‚Üí permite combinar prompt ‚Üí modelo en una sola unidad ejecutable.

Ejemplo conceptual utilizado:

- Instrucci√≥n del sistema: *"Sos un asistente conciso, exacto y profesional."*
- Instrucci√≥n del usuario: *"Explic√° {tema} en ‚â§ 3 oraciones, con un ejemplo real."*

Luego prob√© agregar **few-shot**, es decir, alg√∫n ejemplo previo dentro del prompt para guiar el estilo del modelo.  
Esto mejor√≥ consistencia y redujo variabilidad en explicaciones m√°s complejas.

#### üìù Conclusiones de esta parte

- ChatPromptTemplate ayuda a evitar prompts largos y repetitivos.
- Few-shot mejora claridad cuando el dominio es espec√≠fico.
- El operador `|` permite componer cadenas limpias, list√≠simas para producci√≥n.
- Separar contenido de instrucciones mejora trazabilidad y debugging en LangSmith.

---

### üß± Parte 3 ‚Äì Salida estructurada (JSON confiable con Pydantic)

Esta secci√≥n fue clave: aprend√≠ a generar **JSON v√°lido y estructurado**, sin depender de ‚Äúprompts fr√°giles‚Äù que piden *"devolv√© JSON por favor"*.

LangChain permite usar:

- `with_structured_output`
- Un modelo Pydantic que define los campos obligatorios
- Validaci√≥n autom√°tica del output del LLM

Trabaj√© con un esquema simple:

- `title`: string  
- `bullets`: lista de puntos

El modelo garantiza:

- Que los campos existan  
- Que el JSON sea v√°lido  
- Que no falten claves  
- Que el formato sea consistente incluso en m√∫ltiples ejecuciones  

Esto evita todo el post-procesamiento tradicional y hace la integraci√≥n MUCHO m√°s robusta.

#### üìù Conclusiones de esta parte

- La salida estructurada es esencial para pipelines autom√°ticos.
- El LLM ya no devuelve ‚Äútexto mezclado con JSON‚Äù, sino un objeto validado.
- Reduce errores y elimina parsing manual.
- Es ideal para res√∫menes, extracci√≥n de informaci√≥n, reports y aplicaciones empresariales.

### üìè Parte 4 ‚Äì M√©tricas, Tokens y Observabilidad con LangSmith

En esta secci√≥n explor√© c√≥mo LangChain env√≠a trazas autom√°ticas a **LangSmith**, permitiendo observar:

- Uso de tokens  
- Tiempos de ejecuci√≥n (latencia)  
- Estructura interna de cada ‚Äúrunnable‚Äù  
- Entrada y salida de cada componente (prompt, llm, parseo, etc.)

Despu√©s de ejecutar una cadena como `prompt | llm`, LangSmith registr√≥ autom√°ticamente:

- Tokens de entrada  
- Tokens de salida  
- Costos estimados  
- Timeline del pipeline

Esto es fundamental para analizar rendimiento y evitar sorpresas en producci√≥n.

#### üìù Reflexiones sobre observabilidad

- Algunos prompts consumen muchos m√°s tokens de lo esperado (especialmente los que tienen few-shot).  
- Reducir el tama√±o del contexto o simplificar instrucciones reduce tokens sin p√©rdida de calidad.  
- LangSmith hace muy f√°cil comparar ejecuciones y detectar prompts problem√°ticos.  
- La trazabilidad es clave cuando las cadenas crecen y se vuelven m√°s complejas (especialmente al integrar RAG).

Esta parte del pr√°ctico permite tener control real sobre los costos y el comportamiento del modelo, algo imprescindible en sistemas basados en LLMs.

### üß™ Parte 5 ‚Äì Mini-tareas guiadas (Traducci√≥n, Resumen, Q&A y Extracci√≥n)

En esta secci√≥n apliqu√© lo aprendido para construir peque√±as funcionalidades √∫tiles usando LLMs con LangChain.  
El objetivo fue practicar *prompting estructurado*, *plantillas*, *salida controlada* y *restricciones de formato*.

---

#### üî§ 1. Traductor determinista con salida estructurada

Implement√© un traductor usando `with_structured_output`, garantizando que la salida respetara un esquema JSON fijo.  
Esto elimina la fragilidad de ‚Äúpedir JSON por prompt‚Äù y depender del formato que el modelo quiera producir.

**Caracter√≠sticas:**
- `temperature=0` para m√°xima estabilidad.  
- Esquema Pydantic con campos `"text"` y `"lang"`.  
- Prompt simple y confiable.

**Resultado:**  
Obtengo siempre un objeto v√°lido con texto traducido y el idioma destino, ideal para integrarlo en un pipeline.

---

#### üìù 2. Resumen ejecutivo con secciones obligatorias

Dise√±√© un prompt capaz de producir un **resumen estructurado**, con tres secciones:

- Introducci√≥n  
- Hallazgos  
- Recomendaci√≥n  

Esto muestra c√≥mo LangChain permite **forzar formato** sin necesidad de parseadores externos ni regex.  
La consistencia de estilo entre ejecuciones es muy superior a la de un prompt libre.

---

#### ‚ùì 3. Q&A con contexto ‚Äúen crudo‚Äù

Cre√© un mini-sistema de pregunta‚Äìrespuesta donde:

- Se provee un bloque de **contexto textual**.  
- El modelo **solo puede responder usando ese contexto**.  
- Si no alcanza, debe devolver *"No suficiente contexto"*.

Este peque√±o ejercicio refleja los l√≠mites del prompting sin RAG:  
si el contexto no contiene la informaci√≥n, el modelo ‚Äúadivina‚Äù.  
Imponer esta regla obliga al modelo a declarar insuficiencia de evidencia.

---

#### üóÇÔ∏è 4. Extracci√≥n de informaci√≥n (NER simplificado)

Finalmente, implement√© un extractor estructurado que identifica:

- T√≠tulo  
- Fecha  
- Entidades (ORG / PER / LOC)

Usando un esquema Pydantic, el modelo produce un JSON limpio con campos obligatorios y opcionales.  
Esta t√©cnica es clave cuando se necesita *automatizar pipelines de datos* basados en texto libre.

---

### üìù Reflexiones de esta parte

- La salida estructurada es uno de los **superpoderes** de LangChain: elimina casi todo el post-processing manual.  
- Los errores normales (como formato inconsistentes) desaparecen cuando se usa un esquema formal.  
- Q&A sin RAG tiene l√≠mites claros: si el contexto es pobre, el modelo rellena con supuestos.  
- Los res√∫menes con plantillas mejoran la consistencia y facilitan controlar estilo y longitud.  
- Estas mini-tareas son peque√±as piezas que luego se reutilizan para construir agentes, chatbots y pipelines m√°s complejos.

### üß™ Parte 6 ‚Äì Zero-shot vs Few-shot

En esta secci√≥n compar√© dos enfoques fundamentales en prompting:

- **Zero-shot:** el modelo recibe solo una instrucci√≥n general.
- **Few-shot:** el modelo recibe ejemplos previos que gu√≠an su comportamiento.

El objetivo fue evaluar c√≥mo cambia la consistencia del modelo ante tareas de **clasificaci√≥n de sentimiento**.

---

#### üß™ Zero-shot

En el enfoque zero-shot, el modelo recibe √∫nicamente:

- Una instrucci√≥n clara.
- El texto a clasificar.
- Sin ejemplos previos de c√≥mo debe verse la salida.

**Resultados observados:**

- Para textos muy positivos o muy negativos, acierta con buena precisi√≥n.
- Para textos neutrales, tiende a variar m√°s entre ejecuciones.
- El formato de salida no siempre es estable (puede agregar explicaci√≥n adicional).

Esto muestra que sin ejemplos, el modelo depende √∫nicamente de su conocimiento previo y de la claridad del prompt.

---

#### üß™ Few-shot (1‚Äì2 ejemplos)

Luego defin√≠ una plantilla con **dos ejemplos etiquetados**:

- Un caso claramente positivo.  
- Un caso claramente negativo.

La tercera entrada ‚Äîla que yo quer√≠a clasificar‚Äî segu√≠a el mismo formato.

**Resultados observados:**

- Mayor consistencia en el formato (devuelve solo la etiqueta).
- Menos variabilidad entre ejecuciones incluso con temperature > 0.
- Mejor manejo de los casos ambiguos, especialmente los neutrales.
- Menos propensi√≥n a extenderse en explicaciones.

El few-shot act√∫a como un *molde* que el modelo imita, reduciendo ambig√ºedad.

---

### üìù Comparaci√≥n Zero-shot vs Few-shot

| Aspecto | Zero-shot | Few-shot |
|--------|-----------|----------|
| Consistencia en el formato | Baja | Alta |
| Variabilidad entre ejecuciones | Alta | Muy baja |
| Manejo de ambig√ºedad | Regular | Mejor |
| Estabilidad en tareas evaluables | Baja | Alta |
| Control del estilo | Limitado | Excelente |

---

### üß† Reflexiones de esta parte

- El few-shot funciona como una **demostraci√≥n expl√≠cita del comportamiento esperado**, y el modelo lo replica con gran precisi√≥n.
- Para tareas de clasificaci√≥n con etiquetas cerradas (POS/NEG/NEU), few-shot es significativamente m√°s confiable.
- En cambio, zero-shot es √∫til para rapidez o cuando no se dispone de ejemplos representativos.
- La elecci√≥n entre ambos depende del costo, la necesidad de consistencia y el tipo de tarea.

Esta secci√≥n demuestra por qu√© los ejemplos son una herramienta tan poderosa en prompting y c√≥mo pueden transformar la estabilidad del sistema sin necesidad de entrenamiento adicional.

### üß© Parte 7 ‚Äì Res√∫menes: Single-doc y Map-Reduce

En esta secci√≥n explor√© c√≥mo LangChain permite construir **pipelines de resumen** tanto para textos individuales como para m√∫ltiples documentos usando la estrategia *map-reduce*.

El objetivo fue observar c√≥mo cambia la calidad del resumen cuando:

1. Se resume un texto completo directamente.  
2. Se fragmenta el texto, se resumen los fragmentos (*map*) y luego se combinan (*reduce*).

---

#### üìÑ Resumen de un solo documento

Comenc√© definiendo un texto largo y aplicando un resumen directo.  
Esto permite evaluar:

- Si el modelo mantiene coherencia global.
- Qu√© tan bien conserva las ideas principales.
- Si respeta l√≠mites como ‚Äú<=120 tokens‚Äù o ‚Äúen 3 bullets‚Äù.

**Observaciones:**

- En textos cortos o medianos, el modelo realiza un buen resumen directo.
- En textos largos, tiende a omitir detalles relevantes o mezclar ideas distantes.
- Cuando el texto supera el contexto disponible del modelo, comienza a hallucinar o inventar detalles.

Esto motiv√≥ el uso del enfoque *map-reduce*.

---

#### üóÇÔ∏è Resumen Map-Reduce (chunking + combinaci√≥n)

Luego divid√≠ el texto largo en fragmentos manejables mediante un *text splitter*.

Para cada fragmento:

- Se aplic√≥ un prompt que generaba **2‚Äì3 bullets claros y factuales**.
- Estos bullets se acumularon como resultados parciales (*map stage*).

Finalmente, en el paso *reduce*:

- Todos los bullets se consolidaron eliminando redundancias.
- Se gener√≥ un **resumen final conciso**, con l√≠mite de tokens.

---

### üìù Comparaci√≥n de ambos enfoques

| Criterio | Resumen directo | Map-Reduce |
|----------|------------------|------------|
| Manejo de textos largos | Regular | Excelente |
| Nivel de detalle | Medio | Alto (luego sintetiza) |
| Riesgo de hallucinaciones | Mayor | Menor (usa partes concretas) |
| Coherencia global | Buena | Muy buena (con reduce bien dise√±ado) |
| Costo computacional | Menor | Mayor |

---

### üß† Reflexiones de esta parte

- *Map-reduce* es claramente superior para textos extensos o documentos m√∫ltiples.
- La calidad del resumen depende fuertemente del **splitter** (chunk_size y overlap).
- El paso *reduce* permite controlar el estilo final del resumen:
  - M√°s ejecutivo  
  - M√°s t√©cnico  
  - M√°s narrativo  

- Este patr√≥n es el mismo que utilizan sistemas avanzados de RAG para generar respuestas consistentes basadas en m√∫ltiples documentos.

Esta parte fue clave para comprender c√≥mo escalar res√∫menes y preparar pipelines para sistemas de QA y RAG m√°s robustos.

### üß± Parte 8 ‚Äì Extracci√≥n de informaci√≥n (IE) con Salida Estructurada

En esta parte trabaj√© con **extracci√≥n de informaci√≥n (Information Extraction)** utilizando *salida estructurada* garantizada mediante `with_structured_output`.  
El objetivo fue obtener datos precisos desde texto libre, evitando parsing manual o formatos inconsistentes.

---

#### üéØ Objetivo

Tomar un texto y extraer:

- T√≠tulo (si lo hay)
- Fecha (si aparece expl√≠cita o impl√≠cita)
- Entidades nombradas (PERSONA, ORGANIZACI√ìN, LUGAR)

Usando un esquema Pydantic, el modelo queda obligado a devolver **JSON v√°lido**, cumpliendo tipos y estructura.  
Esto mejora dr√°sticamente la confiabilidad comparado con pedir *"respond√© en formato JSON"*.

---

### üèóÔ∏è Dise√±o del esquema

Defin√≠ dos modelos:

- **Entidad(tipo, valor)** ‚Üí para cada entidad detectada  
- **ExtractInfo(titulo, fecha, entidades[])** ‚Üí estructura principal

Este enfoque garantiza:

- Campos siempre presentes (con opci√≥n de ser null/None si no existe el dato).
- Valores tipados y parseables.
- Respuestas consistentes, sin necesidad de regex o limpieza manual.

---

### üß™ Ejemplo aplicado

Us√© un texto como:

> ‚ÄúOpenAI anunci√≥ una colaboraci√≥n con la Universidad Cat√≥lica del Uruguay en Montevideo el 05/11/2025.‚Äù

El modelo devolvi√≥ informaci√≥n estructurada similar a:

- **titulo:** inferido a partir del evento
- **fecha:** "05/11/2025"
- **entidades:**  
  - ORG ‚Üí OpenAI  
  - ORG ‚Üí Universidad Cat√≥lica del Uruguay  
  - LOC ‚Üí Montevideo  

Esto demostr√≥ que el LLM es capaz no solo de entender el contenido, sino de categorizarlo seg√∫n un esquema formal.

---

### üìù Observaciones de esta parte

- La salida estructurada evita errores comunes como llaves mal cerradas, JSON inv√°lido o formatos ambiguos.
- El LLM realiza una combinaci√≥n de comprensi√≥n sem√°ntica y NER (Named Entity Recognition), produciendo resultados consistentes aun sin un modelo entrenado espec√≠ficamente para NER.
- Si el texto no incluye fecha o t√≠tulo, el modelo rellena con `null`, manteniendo integridad del schema.
- Es un patr√≥n ideal para:
  - Formularios autom√°ticos  
  - Extracci√≥n de datos desde emails  
  - Preprocesamiento para pipelines legales o financieros  
  - Limpieza de informaci√≥n para bases de conocimiento  

---

### üß† Reflexiones

- En tareas de extracci√≥n, el **structured output** es esencial: reduce errores y simplifica todo el pipeline posterior.
- El modelo puede fallar en casos ambiguos (por ejemplo, fechas impl√≠citas), pero el esquema ayuda a detectar f√°cilmente esos fallos.
- Este patr√≥n es el bloque fundamental para construir:
  - Sistemas de ingesti√≥n documental  
  - Motores de b√∫squeda sem√°ntica  
  - Aplicaciones de RAG con metadatos enriquecidos  

### üîé Parte 9 ‚Äì RAG b√°sico con textos locales

En esta secci√≥n constru√≠ un **pipeline RAG minimalista**, sin fuentes externas, usando √∫nicamente:

- Textos locales (peque√±o corpus manual)
- Embeddings de OpenAI
- Un vector store FAISS
- Recuperaci√≥n + generaci√≥n mediante LangChain

El objetivo fue entender c√≥mo funciona RAG desde cero, sin atajos ni magia oculta.

---

### üìö Construcci√≥n del mini-corpus local

Cre√© un conjunto reducido de documentos, por ejemplo:

- ‚ÄúLangChain soporta structured output‚Ä¶‚Äù
- ‚ÄúRAG combina recuperaci√≥n + generaci√≥n‚Ä¶‚Äù
- ‚ÄúOpenAIEmbeddings permite indexar textos‚Ä¶‚Äù

Cada documento se encapsul√≥ como `Document(page_content=...)`.

Luego apliqu√© un **text splitter** para generar chunks de ~300 caracteres con solapamiento, lo que mejora la recuperaci√≥n en textos cortos.

---

### üß† Embeddings + Vector Store

Utilic√©:

- **OpenAIEmbeddings** para representar sem√°nticamente cada chunk.  
- **FAISS** como √≠ndice vectorial local, r√°pido y eficiente.

Esto permiti√≥ realizar b√∫squedas sem√°nticas sin depender de servicios externos.

El `retriever` se configur√≥ con `k=4` para recuperar los 4 fragmentos m√°s relevantes al hacer una pregunta.

---

### üß© Cadena RAG

El pipeline se construy√≥ as√≠:

1. **Retriever** ‚Üí obtiene los fragmentos m√°s relevantes.  
2. **Prompt de combinaci√≥n** ‚Üí un template que dice:  
   ‚ÄúRespond√© SOLO usando el contexto. Si no alcanza, dec√≠ ‚ÄòNo suficiente contexto‚Äô.‚Äù  
3. **LLM (gpt-5-mini)** ‚Üí genera la respuesta final basada exclusivamente en el contexto recuperado.

Este enfoque fuerza un comportamiento grounded, evitando alucinaciones.

---

### üß™ Ejemplo de pregunta

Dada la consulta:

> ‚Äú¬øQu√© ventaja clave aporta RAG?‚Äù

El sistema recuper√≥ los fragmentos relevantes y produjo una respuesta del estilo:

- ‚ÄúRAG aporta grounding, combinando recuperaci√≥n + generaci√≥n para mejorar precisi√≥n y reducir alucinaciones.‚Äù

La respuesta se bas√≥ √∫nicamente en el contenido del mini-corpus, tal como exig√≠a el prompt.

---

### üìù Observaciones de esta parte

- Con un corpus tan peque√±o, los resultados son muy precisos, ya que las posibilidades son limitadas.  
- El comportamiento ‚ÄúNo suficiente contexto‚Äù es crucial para distinguir cu√°ndo el sistema puede responder y cu√°ndo no.  
- Ajustar `k` cambia significativamente la calidad: valores altos pueden introducir ruido; valores bajos pueden dejar fuera informaci√≥n relevante.  
- Esta estructura es la base exacta de sistemas RAG m√°s avanzados que integran PDFs, bases de conocimiento o web search.

---

### üß† Reflexiones

- Incluso un RAG minimalista muestra por qu√© este patr√≥n es superior al prompting directo cuando hay conocimiento espec√≠fico.  
- El control expl√≠cito del contexto evita que el modelo ‚Äúinvente‚Äù.  
- Este ejercicio es un paso esencial antes de construir chatbots, asistentes corporativos o sistemas de soporte con bases de conocimiento reales.

### ü§ñ Desaf√≠o Integrador ‚Äî Chatbot de Soporte ‚ÄúFAQ + WebSearch‚Äù

Para cerrar el pr√°ctico, implement√© un **mini-chatbot de soporte** combinando tres componentes fundamentales:

1. **Un corpus local** con informaci√≥n del producto o dominio.  
2. **Un vector store FAISS** para recuperaci√≥n sem√°ntica basada en embeddings.  
3. **Un LLM estructurado** encargado de generar respuestas finales con fuentes y nivel de confianza.  

Este desaf√≠o aplica todo lo aprendido: prompting, structured output, RAG b√°sico y plantillas con LangChain.

---

### üìö 1. Corpus local (FAQs)

Para este ejercicio prepar√© un peque√±o set de documentos representando informaci√≥n interna, por ejemplo:

- C√≥mo funciona cierto m√≥dulo o servicio  
- Preguntas frecuentes del usuario  
- Problemas comunes y soluciones  
- Definiciones o conceptos clave

Estos documentos fueron convertidos en objetos `Document` y luego divididos en chunks mediante `RecursiveCharacterTextSplitter`.

---

### üß† 2. Indexaci√≥n con Embeddings + FAISS

Utilic√©:

- **OpenAIEmbeddings** para transformar cada chunk en un vector sem√°ntico.  
- **FAISS** como base vectorial local para consultas r√°pidas y eficientes.  

El retriever se configur√≥ con `k=4` para obtener los fragmentos m√°s relevantes ante cada pregunta del usuario.

Este paso establece la base RAG del sistema, donde la recuperaci√≥n es responsable del grounding del modelo.

---

### üß© 3. Template para la respuesta final

Luego dise√±√© un `ChatPromptTemplate` que combina:

- Pregunta del usuario  
- Fragmentos recuperados  
- (Opcional) Resultados de b√∫squeda web  
- Reglas para evitar alucinaciones  
- Formato requerido de salida  

La plantilla exige que el asistente responda **solo** en base al contexto proporcionado y declare expl√≠citamente si la informaci√≥n no alcanza.

---

### üß± 4. Salida estructurada con Pydantic

Para garantizar una respuesta confiable, defin√≠ un esquema:

```json
{
  "answer": "...",
  "sources": [
    {"title": "...", "url": "..."}
  ],
  "confidence": "low|medium|high"
}
```

Luego us√© with_structured_output(...) para que el LLM produzca exactamente ese formato.

Esto permite integrarlo f√°cilmente en aplicaciones reales de soporte, dashboards o APIs.

### üß™ 5. Funcionamiento del chatbot

El flujo final qued√≥ as√≠:

   - Usuario hace una pregunta.
   - El sistema recupera los chunks m√°s relevantes desde el vector store.
   - (Opcional) Realiza web search si el corpus no contiene suficiente informaci√≥n.
   - El LLM genera la respuesta estructurada:
   - Texto final para el usuario
   - Fuentes citadas
   - Nivel de confianza
   - Este patr√≥n es el mismo utilizado en asistentes modernos empresariales.

### üìù Reflexiones finales

El sistema muestra por qu√© RAG + structured output es el est√°ndar actual en chatbots fiables.

La recuperaci√≥n local evita alucinaciones y reduce costos.

El agregado de WebSearch permite resolver casos donde el corpus es insuficiente.

La estructura JSON permite integrar el bot sin fricciones en otros servicios.

Este desaf√≠o conecta perfectamente con lo que viene despu√©s: RAG completo, agentes, y tool use.

### üì∏ Evidencia

Notebook del desarrollo completo (incluyendo el RAG minimalista y el chatbot):

[üìò Enlace al Notebook de Google Colab](https://colab.research.google.com/drive/1UfOP7aYD4-RUWG0lLadMHoB_zUtAA5V0?usp=sharing)