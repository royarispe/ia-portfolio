---
title: "CÃ³mo mejorar el modelo del Titanic con Feature Engineering y un primer modelo base"
date: 
---

# CÃ³mo mejorar el modelo del Titanic con Feature Engineering y un primer modelo base

---

## ğŸ“ Contexto

En la prÃ¡ctica anterior trabajÃ© el EDA del Titanic, mirando el dataset, entendiendo las variables y empezando a sacar las primeras conclusiones.  
En esta prÃ¡ctica el foco cambia: ahora me meto en **feature engineering** y en armar un **primer modelo base** que me sirva de referencia para saber si voy bien encaminado o no.

La idea es pasar de â€œexplorar datosâ€ a â€œpreparar datos para un modelo realâ€ y comparar una RegresiÃ³n LogÃ­stica con un baseline muy simple.

---

## ğŸ¯ Objetivos

En esta prÃ¡ctica busco:

- Entender mejor algunos componentes de Scikit-learn: `LogisticRegression`, `DummyClassifier`, `train_test_split` y las mÃ©tricas de clasificaciÃ³n.
- Aplicar **feature engineering simple** sobre el mismo dataset del Titanic para generar nuevas columnas que aporten informaciÃ³n.
- Entrenar un modelo baseline y un modelo de RegresiÃ³n LogÃ­stica, y compararlos.
- Mirar la matriz de confusiÃ³n y el classification report para entender **cÃ³mo** se equivoca el modelo, no solo cuÃ¡nto acierta.

---

## ğŸš€ Desarrollo

### 0ï¸âƒ£ InvestigaciÃ³n rÃ¡pida de Scikit-learn

Antes de tocar cÃ³digo me detuve a leer un poco la documentaciÃ³n oficial de Scikit-learn:

- `LogisticRegression`:  
  Sirve para problemas de **clasificaciÃ³n binaria**, como este caso de sobreviviÃ³ / no sobreviviÃ³. Me quedo con que parÃ¡metros como `solver`, `C`, `penalty` y `max_iter` son importantes. El solver `liblinear` es una buena opciÃ³n cuando tengo datasets no tan grandes y varias variables binarias.

- `DummyClassifier`:  
  Es un modelo â€œtontoâ€ a propÃ³sito. Sirve como **baseline**, para ver quÃ© pasa si siempre predigo la clase mÃ¡s frecuente o si predigo al azar. Si mi modelo â€œserioâ€ no supera esto, es seÃ±al de alarma.

- `train_test_split`:  
  Se usa para separar en train y test.  
  - `stratify=y` lo uso para mantener las proporciones de clases.  
  - `random_state` me permite repetir los resultados.  
  - En este caso usÃ© `test_size=0.2`, que es bastante estÃ¡ndar.

- MÃ©tricas de evaluaciÃ³n:  
  El `classification_report` me da precision, recall y f1-score por clase. La **matriz de confusiÃ³n** me muestra exactamente cuÃ¡ntos aciertos y errores hay en cada categorÃ­a. La accuracy estÃ¡ buena, pero en problemas desbalanceados no alcanza con mirarla solo a ella.

---

### 1ï¸âƒ£ Feature Engineering

A partir del dataset `train` hice una copia en `df` y trabajÃ© sobre esa. Los pasos fueron:

#### ğŸ§¹ Manejo de valores faltantes (imputaciÃ³n)

- `Embarked`: reemplacÃ© los nulos por la **moda** (el valor mÃ¡s frecuente).  
- `Fare`: reemplacÃ© los nulos por la **mediana**.  
- `Age`: imputÃ© la edad agrupando por `Sex` y `Pclass`, usando la mediana de cada grupo.

La intenciÃ³n es que el modelo no pierda filas por valores faltantes, pero que la imputaciÃ³n tambiÃ©n tenga algo de sentido.

#### ğŸ§± CreaciÃ³n de nuevas features

DespuÃ©s generÃ© algunas columnas nuevas:

- `FamilySize = SibSp + Parch + 1` â†’ tamaÃ±o de la familia viajando junta.  
- `IsAlone` â†’ una bandera que indica si la persona viajaba sola (`FamilySize == 1`).  
- `Title` â†’ extraÃ­do desde la columna `Name`, y luego agrupÃ© tÃ­tulos poco frecuentes bajo la categorÃ­a `Rare`.

Estas features nuevas intentan capturar informaciÃ³n mÃ¡s â€œhumanaâ€ del problema: si viajÃ¡s solo, con familia, tu tÃ­tulo social, etc.

#### ğŸ”„ PreparaciÃ³n final para el modelo

DefinÃ­ la lista de columnas a usar como features:

```python
features = ['Pclass','Sex','Age','Fare','Embarked',
            'FamilySize','IsAlone','Title','SibSp','Parch']
```

Luego hice pd.get_dummies para pasar las variables categÃ³ricas a numÃ©ricas y definÃ­:

- X â†’ features ya codificadas
- y â†’ la variable objetivo Survived

El resultado final fue:

- X: 891 filas y 14 columnas
- y: 891 registros de la variable objetivo

---

### 2ï¸âƒ£ Modelo base y baseline

AcÃ¡ entran en juego los modelos.

Primero separÃ© el dataset en train y test:

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

#### ğŸ¯ Baseline â€” DummyClassifier

EntrenÃ© un DummyClassifier con estrategia most_frequent, que bÃ¡sicamente siempre predice la clase mayoritaria (â€œno sobreviviÃ³â€). Esto es mi piso.

#### ğŸ¤– Modelo principal â€” LogisticRegression

Luego entrenÃ© una LogisticRegression con:

- max_iter=1000
- solver='liblinear'
- random_state=42

Finalmente comparÃ© accuracies, el classification report y la matriz de confusiÃ³n.

Los resultados que obtuve fueron:

- Baseline accuracy: ~0.61
- Logistic Regression accuracy: ~0.82
- La RegresiÃ³n LogÃ­stica mejora bastante el baseline, lo cual tiene sentido porque estÃ¡ usando toda la informaciÃ³n de las features nuevas.

## â“ Preguntas finales

ğŸ”¸ 1. Matriz de confusiÃ³n: Â¿en quÃ© casos se equivoca mÃ¡s el modelo?

La matriz de confusiÃ³n del modelo de RegresiÃ³n LogÃ­stica fue:

- Verdaderos negativos: 98
- Falsos positivos: 12
- Falsos negativos: 21
- Verdaderos positivos: 48

El modelo se equivoca mÃ¡s cuando predice que alguien NO sobreviviÃ³ pero en realidad sÃ­ sobreviviÃ³ (falsos negativos = 21), frente a los falsos positivos (12).

ğŸ”¸ 2. Clases atendidas: Â¿acierta mÃ¡s en los que sobrevivieron o en los que no?

Acierta mÃ¡s en la clase â€œno sobreviviÃ³â€, porque:

- Tiene mÃ¡s ejemplos en el dataset.
- Es la clase mayoritaria, asÃ­ que el modelo tiende a predecirla mejor.

En la prÃ¡ctica, eso se ve tanto en la matriz como en el classification report.

ğŸ”¸ 3. ComparaciÃ³n con baseline

SÃ­, la RegresiÃ³n LogÃ­stica supera ampliamente al baseline:

- Baseline (siempre clase mÃ¡s frecuente): ~61%
- RegresiÃ³n LogÃ­stica: ~82%

Esto me confirma que el feature engineering + el modelo logÃ­stico estÃ¡n captando patrones reales del problema y no solo â€œrepitiendo la clase que mÃ¡s apareceâ€.

ğŸ”¸ 4. Â¿QuÃ© error es mÃ¡s grave para este problema?

En este contexto, podrÃ­amos discutirlo, pero personalmente me parece mÃ¡s grave el falso negativo: cuando el modelo dice â€œno sobreviveâ€ y en realidad la persona sÃ­ sobreviviÃ³.
En una versiÃ³n mÃ¡s seria del problema (por ejemplo, simulaciones para protocolos de evacuaciÃ³n), subestimar la supervivencia de ciertas personas podrÃ­a llevar a decisiones injustas o peligrosas.

ğŸ”¸ 5. Observaciones generales

- Las nuevas features (FamilySize, IsAlone, Title) aportan informaciÃ³n y ayudan al modelo.
- La clase y el sexo siguen siendo variables muy relevantes.
- El salto desde el baseline (~0.61) hasta la LogReg (~0.82) muestra lo importante del feature engineering.

ğŸ”¸ 6. Mejoras simples para el futuro

Una idea clara de mejora es trabajar con la informaciÃ³n de la cabina (Cabin), por ejemplo extrayendo la letra de la cabina como indicador de zona del barco. TambiÃ©n se podrÃ­a seguir refinando los tÃ­tulos (Title) y hacer mÃ¡s pruebas de combinaciÃ³n de features.

## ğŸ“¸ Evidencias

[Enlace al notebook](https://colab.research.google.com/drive/13l-o1ZTeMMskTwmTqSLBpcZ3vQ1-3veP?usp=sharing)

### ğŸ“Š Salidas clave

CÃ³digo de feature engineering, preparaciÃ³n de X e y mÃ¡s resultado:

![Feature Engineering](../assets/ut1_p2_1.png){ width="480" }

Accuracy del baseline y de la RegresiÃ³n LogÃ­stica + classification_report junto con Matriz de confusiÃ³n del modelo de RegresiÃ³n LogÃ­stica:

![Baseline vs LogReg + Matriz de confusiÃ³n](../assets/ut1_p2_2.png){ width="480" }

## ğŸ’¡ ReflexiÃ³n

Este prÃ¡ctico me ayudÃ³ a ver en la prÃ¡ctica algo que muchas veces se dice de memoria:
el modelo es importante, pero la preparaciÃ³n de los datos lo es todavÃ­a mÃ¡s.

Pasar de un baseline muy tonto a una RegresiÃ³n LogÃ­stica bien alimentada con features creadas a mano cambia por completo el rendimiento. Me gustÃ³ ver cÃ³mo decisiones bastante simples (imputar bien, crear columnas nuevas, codificar categorÃ­as) tienen un impacto directo en los resultados.

Siento que, sumando este prÃ¡ctico al anterior, ya tengo una idea mÃ¡s concreta del flujo:

- Entender los datos (EDA).
- Prepararlos y enriquecerlos (feature engineering).
- Probar un modelo base y compararlo con un baseline.

Esto me da una base real para despuÃ©s pasar a modelos mÃ¡s complejos sin hacer â€œmagia negraâ€.

## ğŸ“š Referencias

- [DocumentaciÃ³n de Scikit-learn â€“ Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
- [DocumentaciÃ³n de Scikit-learn â€“ DummyClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier)
- [DocumentaciÃ³n de Scikit-learn â€“ train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)