---
title: "Agentes con LangGraph: RAG, Tools y Memoria Conversacional"
date:
---

# Agentes con LangGraph: RAG, Tools y Memoria Conversacional

---

## üìù Contexto

En este pr√°ctico trabaj√© con **LangGraph**, una librer√≠a orientada a la construcci√≥n de **agentes conversacionales estructurados** mediante grafos de estado.  
El foco estuvo en ir m√°s all√° del uso directo de un LLM, construyendo un **agente multi-turn** capaz de:

- Mantener estado entre turnos.
- Decidir cu√°ndo responder directamente y cu√°ndo llamar herramientas (*tools*).
- Integrar **RAG** como una tool reutilizable.
- Incorporar una **memoria ligera** para resumir la conversaci√≥n.
- Orquestar todo mediante un **grafo expl√≠cito** de ejecuci√≥n.

El notebook del Colab contiene el pipeline completo.  
Las llamadas al modelo no se ejecutan actualmente debido a que las **credenciales provistas originalmente para la cursada ya expiraron**, pero el c√≥digo queda listo para funcionar sin cambios al reponer las API keys.

---

## üéØ Objetivos

Los objetivos principales de este pr√°ctico fueron:

- Comprender el modelo mental de **LangGraph** (estado + nodos + transiciones).
- Dise√±ar un `AgentState` expl√≠cito para conversaciones multi-turn.
- Construir un agente que combine:
  - razonamiento con LLM,
  - recuperaci√≥n de conocimiento (RAG),
  - tools auxiliares.
- Entender c√≥mo los **tool calls** afectan el flujo de ejecuci√≥n.
- Implementar memoria conversacional ligera mediante res√∫menes.
- Explorar patrones reales usados en agentes de soporte y asistentes inteligentes.

---

## üöÄ Desarrollo

### ü§ñ Parte 0 ‚Äì Setup y primer agente m√≠nimo

Comenc√© con un **agente LangGraph m√≠nimo**, definiendo:

- Un estado b√°sico (`messages`) que acumula el historial.
- Un √∫nico nodo `assistant` que llama al modelo con todo el historial.
- Un flujo lineal `START ‚Üí assistant ‚Üí END`.

Este primer paso permiti√≥ entender la diferencia clave entre:

- `llm.invoke("prompt")`
- y un **estado que viaja por un grafo**, siendo modificado en cada nodo.

‚úî El agente ya es *stateful*, incluso en su versi√≥n m√°s simple.

---

### üß± Parte 1 ‚Äì Estado del agente con memoria ligera

Luego extend√≠ el estado del agente agregando:

- `messages`: historial completo.
- `summary`: un resumen corto de la conversaci√≥n.

Esta memoria ligera est√° pensada para:

- Reducir el contexto enviado al modelo.
- Mantener informaci√≥n clave sin reenviar todo el historial.
- Preparar el agente para conversaciones largas.

Aunque el resumen no es obligatorio en cada ejecuci√≥n, dejar el estado preparado permite escalar el dise√±o sin refactorizaciones posteriores.

---

### üìö Parte 2 ‚Äì Construcci√≥n de un RAG mini como tool

En esta etapa arm√© un **RAG minimalista** con textos locales:

- Un peque√±o corpus manual.
- Split en chunks con solapamiento.
- Embeddings de OpenAI.
- Vector store FAISS.

El objetivo no fue maximizar performance, sino **entender el patr√≥n RAG desde cero** y convertirlo en una **tool reutilizable** (`rag_search`) que el agente pueda invocar cuando lo necesite.

Este enfoque refleja c√≥mo se integran bases de conocimiento internas en agentes reales.

---

### üõ†Ô∏è Parte 3 ‚Äì Tool adicional no-RAG

Adem√°s del RAG, agregu√© tools auxiliares simples, por ejemplo:

- Consulta de estado de pedidos ficticios.
- Obtenci√≥n de la hora actual.

Estas tools simulan **servicios externos** t√≠picos de un agente de soporte, y sirven para observar c√≥mo el LLM decide cu√°ndo delegar una respuesta a una herramienta.

---

### üß† Parte 4 ‚Äì Tool calling y ToolNode

Aqu√≠ se dio el salto conceptual m√°s importante del pr√°ctico:

- El LLM se *bindea* con una lista de tools.
- El agente puede responder directamente o emitir `tool_calls`.
- Un `ToolNode` ejecuta las tools solicitadas.
- El flujo vuelve al nodo `assistant`.

El grafo queda con un bucle expl√≠cito:

assistant ‚Üî tools

Esto hace visible algo que normalmente queda oculto en frameworks m√°s ‚Äúautom√°ticos‚Äù:  
üëâ **el razonamiento del agente y sus decisiones de control de flujo**.

---

### üí¨ Parte 5 ‚Äì Conversaci√≥n multi-turn

Prob√© el agente en m√∫ltiples turnos:

- Primer mensaje: pregunta conceptual.
- Segundo mensaje: consulta que requiere usar RAG.
- Observaci√≥n de c√≥mo el estado evoluciona entre ejecuciones.

El mismo grafo se reutiliza, pero el **estado ya no es el inicial**, sino el resultado del turno anterior.

Este patr√≥n es la base de cualquier asistente conversacional real.

---

### üß™ Parte 6 ‚Äì Memoria conversacional con summary (opcional)

De forma opcional, agregu√© un nodo `memory_node` que:

- Lee el historial reciente.
- Genera un resumen en pocos bullets.
- Actualiza `state["summary"]`.

Este dise√±o permite:

- Controlar cu√°ndo se actualiza la memoria.
- Evitar enviar informaci√≥n sensible o irrelevante.
- Reducir costos y latencia en conversaciones largas.

---

### ‚ö° Parte 7 ‚Äì Interfaz con Gradio

Finalmente, se implement√≥ una **UI simple con Gradio** para:

- Probar el agente sin editar c√≥digo.
- Visualizar el historial de mensajes.
- Ver qu√© tools se activan en cada respuesta.
- Mantener el estado entre interacciones.

Esta interfaz acelera la experimentaci√≥n y facilita la detecci√≥n de errores en el comportamiento del agente.

---

### üì∏ Evidencia

Notebook del desarrollo completo (incluyendo el RAG minimalista y el chatbot):

[üìò Enlace al Notebook de Google Colab](https://colab.research.google.com/drive/1rxQc42roHYtwAHZ41DyDDdx2wxK4mQ30?usp=sharing)

> Nota: las credenciales de OpenAI utilizadas durante la cursada ya no est√°n activas, por lo que el c√≥digo no ejecuta inferencias actualmente. El pipeline queda listo para funcionar al reponer las API keys correspondientes.

---

## üß† Reflexi√≥n Final

Este pr√°ctico marc√≥ un punto de inflexi√≥n respecto al uso tradicional de LLMs:

- Pas√© de **prompts aislados** a **agentes con estado expl√≠cito**.
- El uso de LangGraph hace visible el flujo de razonamiento y control.
- RAG como tool muestra por qu√© este patr√≥n es esencial para respuestas confiables.
- La memoria ligera introduce preocupaciones reales de escalabilidad y privacidad.
- La separaci√≥n entre reasoning, tools y memoria refleja arquitecturas usadas en producci√≥n.

En conjunto, este ejercicio consolida los fundamentos necesarios para avanzar hacia **agentes m√°s complejos**, con planificaci√≥n, herramientas externas y comportamiento consistente en el tiempo.

## üìö Referencias

![LangGraph Documentation ‚Äì State Graphs for LLM Agents](https://langgraph.langchain.com/)
![LangChain OpenAI Integration ‚Äì ChatOpenAI](https://python.langchain.com/docs/integrations/chat/openai/)
![LangChain Tools ‚Äì Definici√≥n e invocaci√≥n de herramientas](https://python.langchain.com/docs/concepts/tools/)
![Retrieval-Augmented Generation (RAG) ‚Äì Conceptos y patrones](https://python.langchain.com/docs/concepts/rag/)
![FAISS Vector Store ‚Äì B√∫squeda vectorial local](https://github.com/facebookresearch/faiss)
![LangChain Text Splitters ‚Äì Chunking de documentos](https://python.langchain.com/docs/concepts/text_splitters/)
![LangGraph ToolNode ‚Äì Ejecuci√≥n de tools en grafos](https://langgraph.langchain.com/docs/concepts/tool_node/)
![LangChain Memory ‚Äì Manejo de estado y contexto](https://python.langchain.com/docs/concepts/memory/)
![Gradio ‚Äì Interfaces r√°pidas para ML y LLMs](https://www.gradio.app/)
![OpenAI Platform ‚Äì Modelos y par√°metros de generaci√≥n](https://platform.openai.com/docs)
