---
title: "Pr√°ctica 1: EDA del Titanic en Google Colab"
date: 2025-08-12
---

# Pr√°ctica 1: EDA del Titanic en Google Colab

---

## üìù Contexto

> **Nota:**  
    En esta ocasi√≥n como primer acercamiento al trabajo con ML, rama de la IA vamos a trabajar con el dataset del [Titanic](https://www.kaggle.com/competitions/titanic/data), de esta forma a trav√©s de la pr√°ctica comenzamos a ponernos manos a la obra para explorar este mundo en auge dentro de la inform√°tica.

---

## üéØ Objetivos

Explorar, preparar y utilizar herramientas clave para el aprendizaje durante el curso:

- [Google Colab](https://colab.google/)
- [Kaggle](https://www.kaggle.com/)
- [Pandas](https://pandas.pydata.org/docs/)
- [Numpy](https://numpy.org/doc/stable/)
- [Matplotlib](https://matplotlib.org/stable/users/index)
- [Seaborn](https://seaborn.pydata.org/tutorial.html)

---

## ‚è±Ô∏è Actividades y Tiempos Estimados

| Actividad                      | Tiempo      |
|--------------------------------|:----------:|
| **Tarea 1:** Setup en Colab    | 5 min      |
| **Tarea 2:** Cargar dataset    | 5-10 min   |
| **Tarea 3:** Conocer dataset   | 10 min     |
| **Tarea 4:** EDA visual        | 15 min     |
| **Tarea 5:** Preguntas finales | ‚Äî          |

---

## üöÄ Desarrollo

### Tarea 1 ‚úÖ - Setup en Colab

Si bien el pr√°ctico es muy claro y se comparte el c√≥digo correspondiente para poder realizarlo de manera muy r√°pida, como mencion√© en los objetivos mi idea es empaparme de los temas y aprender realmente por lo cual dentro de esta tarea antes de preparar el setup comenc√© por leer la [Documentaci√≥n](https://colab.research.google.com/#scrollTo=vwnNlNIEwoZ8) para entender qu√© era esto de Google Colab, a grandes rasgos es un Notebook, "un libro" digital con c√≥digo dentro, la primera pregunta que me surgi√≥ es: ¬øpor qu√© lo escribimos y ejecutamos en esta plataforma en vez de en local en mi editor de texto? , la respuesta es sencilla, esos Notebooks primero que nada se almacenan directamente en mi Google Drive, haciendo esto f√°cilmente compartible con compa√±eros y es como un Plug and Play, de muy f√°cil uso, pero el verdadero potencial de esto radica en que al ejecutar lo que estoy desarrollando corre en Google Cloud aprovechando las GPU'S y TPU'S de ellos, gracias a esto, aprovechando la potencia del hardware de Google puedo probar todo mucho m√°s r√°pido que utilizando mi m√°quina.

---

### Tarea 2 ‚úÖ - Cargar el dataset de Kaggle

Dataset, no es otra cosa que el conjunto de datos con el cual vamos a trabajar, en este caso, los mismos fueron extra√≠dos del evento del Titanic, con estos, probaremos las posibilidades, una introducci√≥n pr√°ctica a la parte del an√°lisis de datos, ya que esto es la base de todo lo referido a ML'S e IA'S.

Volviendo a la tarea en particular, seguir el c√≥digo es lo f√°cil, en lo que me detuve fue en investigar qu√© es Kaggle, b√°sicamente explotamos el entorno de Google, ya que Kaggle es una plataforma y una comunidad donde aprender, practicar y completar problemas orientados al Data Science, ya que para entrenar modelos, predecir y m√°s la base son los datos que existen a partir de los cuales analizar, buscar patrones y refinar un modelo que prediga el futuro posible. De esta plataforma tomamos los datasets p√∫blicos y disponibles para poder investigar y desarrollar nuestros modelos.

---

### Tarea 3 ‚úÖ - Conocer el dataset

En este caso no hab√≠a mucho que analizar desde cero, ya que dentro del desafio de kaggle te comparten de manera muy detallada cuales son los datos compone el [dataset](https://www.kaggle.com/competitions/titanic/data), qu√© columnas o atributos, qu√© nombre identifica cada uno, lo que representa, los valaros puede tomar, etc.

Lo interesante de esta parte fue el probar el c√≥digo, las funciones disponibles, que ac√° me frene posterior a ver las salidas, para aprender qu√© se est√° utilizando, la librer√≠a de python [Panda](https://pandas.pydata.org/docs/).

Pandas es una librer√≠a open source, con una gran performance utilizada para todo lo referido a la estructura y el an√°lisis de los datos, esta librer√≠a nos ayuda a explorar, limpiar y procesar los datos.

La informaci√≥n que tenemos que manipular (excel, csv, sql, etc) es cargada en una tabla de datos a la cual se la llama como "DataFrame" d√°ndonos la posibilidad tambi√©n de exportar los resultados devuelta a un archivo.

Pandas nos da todas las funcionlidades que necesitamos para esta √°rea de la inform√°tica, filtrar, seleccionar, extraer y m√°s operaciones con los datos. En conjunto con [Matplotlib](https://matplotlib.org/) otra librer√≠a la cual se encarga de todo lo que es la creaci√≥n de visualizaciones est√°ticas, animadas de los datos que procesamos es el combo perfecto para el an√°lisis de los datos.

Como siguiente paso utilc√© la CheatSheet de pandas para un paneo general y la [APIReferences](https://pandas.pydata.org/docs/reference/frame.html) para investigar qu√© hace cada linea que utilic√©:

```python
train.shape  # Tupla con filas y columnas
```
Devuelve una tupla con el n√∫mero de las filas y de columnas que tenemos.

```python
train.columns  # Nombres de columnas
```
Devuelve los atributos que tenemos en el DF, lo que quiere decir los nombres de cada columna que tenemos.

```python
train.head(3)  # Primeras 3 filas
```
Devuelve las primeras 3 rows del DF.

```python
train.info()  # Resumen del dataset
```
Nos da un resumen del dataset (este puede ser modificado en funci√≥n de que par√°metros le pase).

```python
train.describe(include='all').T  # Estad√≠sticas descriptivas
```
Nos devuelve una descripci√≥n estad√≠stica (percentiles, mediana, media, desviaci√≥n estandar, etc) del DF, al enviar como par√°metro el include "all" este an√°lisis incluye todos los valores de las columnas.

```python
train.isna().sum().sort_values(ascending=False)  # Valores nulos ordenados
```
.isna nos da un output con todos los valores boolean donde, todos los valores NA ser√°n True por el contrario ser√°n False.

.sum suma la cantidad de elementos que tenemos.

.sort_values(ascending=False): Ordena el output de manera descendiente.

```python
train['Survived'].value_counts(normalize=True)  # Proporci√≥n de supervivientes
```
En este caso trabajamos con la columna "Survived" y obtenemos un output de la cantidad de supervivientes en formato de proporci√≥n por el atributo "normalize=True", b√°sicamente del total que porcentaje sobrevivi√≥ y cu√°l no.

### Tarea 4 ‚úÖ - EDA visual con Seaborn/Matplotlib

En esta tarea nos adentramos m√°s en la parte visual, que tambi√©n es muy importante para ayudarnos a entender y comunicar sobre resultados de los an√°lisis.

La primera pregunta que me surgi√≥ fue ¬øQu√© es EDA? An√°lisis exploratorio de datos, es un proceso de investigaci√≥n en ciencia de datos que utiliza estad√≠sticas y visualizaciones para explorar conjunto de datos, descubrir patrones e indentificar problemas y generar nuevas preguntas.

En resumen, la idea es entender la estructura de los datos y preparar los mismos para todo tipo de an√°lisis y trabajo con estos datos.

En el caso de esta pr√°ctica, este EDA lo realizamos visual con la librer√≠a [Seaborn](https://seaborn.pydata.org/tutorial.html), esta funciona sobre Matplotlib que ya mencion√© anteriormente e integrando pandas para el trabajo con los datos.

---

## üì∏ Evidencias

[Enlace al notebook](https://colab.research.google.com/drive/1PjFidbLK2lcRPLRYjNs6gpr7ZHA-8Z_o?usp=sharing)

### Salidas clave

![setup inicial](../assets/UT1/P1/1.png){ width="480" }
![kaggle_setup](../assets/UT1/P1/2.png){ width="480" }
![Head del dataset](../assets/UT1/P1/3.png){ width="480" }
![Resumen del dataset](../assets/UT1/P1/4.png){ width="480" }
![Estad√≠sticas descriptivas](../assets/UT1/P1/5.png){ width="480" }
![Valores nulos](../assets/UT1/P1/6.png){ width="480" }
![Proporci√≥n de supervivientes](../assets/UT1/P1/7.png){ width="480" }
![C√≥digo visualizaci√≥n](../assets/UT1/P1/8.png){ width="480" }
![Visualizaciones EDA](../assets/UT1/P1/8.1.png){ width="480" }



---

## üí° Reflexi√≥n

Este primer pr√°ctico me permiti√≥ dar un paso real dentro del mundo del Machine Learning. No solo ejecut√© c√≥digo ya escrito, sino que me detuve a entender qu√© hac√≠a cada l√≠nea, qu√© herramientas usaba y por qu√© eran necesarias. Descubr√≠ la importancia de tener un entorno de trabajo organizado (Colab, Drive, Kaggle) y confirm√© que las librer√≠as como Pandas, Matplotlib y Seaborn son la base de cualquier an√°lisis de datos.

Creo que lo m√°s valioso fue comprender que un EDA no es solo mostrar gr√°ficos, sino aprender a leer los datos, detectar problemas y prepararlos para el siguiente nivel de an√°lisis. Me doy cuenta de que todav√≠a tengo que mejorar en automatizar mis flujos de trabajo y aprovechar m√°s la documentaci√≥n oficial en lugar de depender solo de ejemplos.

Como pr√≥ximos pasos, me gustar√≠a buscar la vuelta para una mejor organizaci√≥n para poder darme mejor el espacio para aprender las cosas, al menos conseguir ese nivel de aprendizaje o conocimiento donde al enfrentar problemas reales poder tener estas herramientas como una posibilidad para aplicar y encontrar soluciones.

---

## üìö Referencias

- [Google Colab Docs](https://colab.research.google.com/#scrollTo=vwnNlNIEwoZ8)
- [Kaggle Titanic Competition](https://www.kaggle.com/competitions/titanic/data)
- [Pandas API Reference](https://pandas.pydata.org/docs/reference/frame.html)
- [Matplotlib Docs](https://matplotlib.org/)
- [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html)