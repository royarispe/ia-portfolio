---
title: "Exploraci√≥n del dataset Titanic: patrones de supervivencia y primeras hip√≥tesis"
date: 
---

# Exploraci√≥n del dataset Titanic: patrones de supervivencia y primeras hip√≥tesis

---

## üìù Contexto

> **Nota:**  
    En esta ocasi√≥n como primer acercamiento al trabajo con ML, rama de la IA vamos a trabajar con el dataset del [Titanic](https://www.kaggle.com/competitions/titanic/data), de esta forma a trav√©s de la pr√°ctica comenzamos a ponernos manos a la obra para explorar este mundo en auge dentro de la inform√°tica.

---

## üéØ Objetivos

Explorar, preparar y utilizar herramientas clave para el aprendizaje durante el curso:

- [Google Colab](https://colab.google/)
- [Kaggle](https://www.kaggle.com/)
- [Pandas](https://pandas.pydata.org/docs/)
- [Numpy](https://numpy.org/doc/stable/)
- [Matplotlib](https://matplotlib.org/stable/users/index)
- [Seaborn](https://seaborn.pydata.org/tutorial.html)

---

## ‚è±Ô∏è Actividades y Tiempos Estimados

| Actividad                      | Tiempo      |
|--------------------------------|:----------:|
| **Tarea 1:** Setup en Colab    | 5 min      |
| **Tarea 2:** Cargar dataset    | 5-10 min   |
| **Tarea 3:** Conocer dataset   | 10 min     |
| **Tarea 4:** EDA visual        | 15 min     |
| **Tarea 5:** Preguntas finales | ‚Äî          |

---

## üöÄ Desarrollo

### Tarea 1 ‚úÖ - Setup en Colab

Si bien el pr√°ctico es muy claro y se comparte el c√≥digo correspondiente para poder realizarlo de manera muy r√°pida, como mencion√© en los objetivos mi idea es empaparme de los temas y aprender realmente por lo cual dentro de esta tarea antes de preparar el setup comenc√© por leer la [Documentaci√≥n](https://colab.research.google.com/#scrollTo=vwnNlNIEwoZ8) para entender qu√© era esto de Google Colab, a grandes rasgos es un Notebook, "un libro" digital con c√≥digo dentro, la primera pregunta que me surgi√≥ es: ¬øpor qu√© lo escribimos y ejecutamos en esta plataforma en vez de en local en mi editor de texto? , la respuesta es sencilla, esos Notebooks primero que nada se almacenan directamente en mi Google Drive, haciendo esto f√°cilmente compartible con compa√±eros y es como un Plug and Play, de muy f√°cil uso, pero el verdadero potencial de esto radica en que al ejecutar lo que estoy desarrollando corre en Google Cloud aprovechando las GPU'S y TPU'S de ellos, gracias a esto, aprovechando la potencia del hardware de Google puedo probar todo mucho m√°s r√°pido que utilizando mi m√°quina.

---

### Tarea 2 ‚úÖ - Cargar el dataset de Kaggle

Dataset, no es otra cosa que el conjunto de datos con el cual vamos a trabajar, en este caso, los mismos fueron extra√≠dos del evento del Titanic, con estos, probaremos las posibilidades, una introducci√≥n pr√°ctica a la parte del an√°lisis de datos, ya que esto es la base de todo lo referido a ML'S e IA'S.

Volviendo a la tarea en particular, seguir el c√≥digo es lo f√°cil, en lo que me detuve fue en investigar qu√© es Kaggle, b√°sicamente explotamos el entorno de Google, ya que Kaggle es una plataforma y una comunidad donde aprender, practicar y completar problemas orientados al Data Science, ya que para entrenar modelos, predecir y m√°s la base son los datos que existen a partir de los cuales analizar, buscar patrones y refinar un modelo que prediga el futuro posible. De esta plataforma tomamos los datasets p√∫blicos y disponibles para poder investigar y desarrollar nuestros modelos.

---

### Tarea 3 ‚úÖ - Conocer el dataset

En este caso no hab√≠a mucho que analizar desde cero, ya que dentro del desafio de kaggle te comparten de manera muy detallada cuales son los datos compone el [dataset](https://www.kaggle.com/competitions/titanic/data), qu√© columnas o atributos, qu√© nombre identifica cada uno, lo que representa, los valaros puede tomar, etc.

Lo interesante de esta parte fue el probar el c√≥digo, las funciones disponibles, que ac√° me frene posterior a ver las salidas, para aprender qu√© se est√° utilizando, la librer√≠a de python [Panda](https://pandas.pydata.org/docs/).

Pandas es una librer√≠a open source, con una gran performance utilizada para todo lo referido a la estructura y el an√°lisis de los datos, esta librer√≠a nos ayuda a explorar, limpiar y procesar los datos.

La informaci√≥n que tenemos que manipular (excel, csv, sql, etc) es cargada en una tabla de datos a la cual se la llama como "DataFrame" d√°ndonos la posibilidad tambi√©n de exportar los resultados devuelta a un archivo.

Pandas nos da todas las funcionlidades que necesitamos para esta √°rea de la inform√°tica, filtrar, seleccionar, extraer y m√°s operaciones con los datos. En conjunto con [Matplotlib](https://matplotlib.org/) otra librer√≠a la cual se encarga de todo lo que es la creaci√≥n de visualizaciones est√°ticas, animadas de los datos que procesamos es el combo perfecto para el an√°lisis de los datos.

Como siguiente paso utilc√© la CheatSheet de pandas para un paneo general y la [APIReferences](https://pandas.pydata.org/docs/reference/frame.html) para investigar qu√© hace cada linea que utilic√©:

```python
train.shape  # Tupla con filas y columnas
```
Devuelve una tupla con el n√∫mero de las filas y de columnas que tenemos.

```python
train.columns  # Nombres de columnas
```
Devuelve los atributos que tenemos en el DF, lo que quiere decir los nombres de cada columna que tenemos.

```python
train.head(3)  # Primeras 3 filas
```
Devuelve las primeras 3 rows del DF.

```python
train.info()  # Resumen del dataset
```
Nos da un resumen del dataset (este puede ser modificado en funci√≥n de que par√°metros le pase).

```python
train.describe(include='all').T  # Estad√≠sticas descriptivas
```
Nos devuelve una descripci√≥n estad√≠stica (percentiles, mediana, media, desviaci√≥n estandar, etc) del DF, al enviar como par√°metro el include "all" este an√°lisis incluye todos los valores de las columnas.

```python
train.isna().sum().sort_values(ascending=False)  # Valores nulos ordenados
```
.isna nos da un output con todos los valores boolean donde, todos los valores NA ser√°n True por el contrario ser√°n False.

.sum suma la cantidad de elementos que tenemos.

.sort_values(ascending=False): Ordena el output de manera descendiente.

```python
train['Survived'].value_counts(normalize=True)  # Proporci√≥n de supervivientes
```
En este caso trabajamos con la columna "Survived" y obtenemos un output de la cantidad de supervivientes en formato de proporci√≥n por el atributo "normalize=True", b√°sicamente del total que porcentaje sobrevivi√≥ y cu√°l no.

### Tarea 4 ‚úÖ - EDA visual con Seaborn/Matplotlib

En esta tarea nos adentramos m√°s en la parte visual, que tambi√©n es muy importante para ayudarnos a entender y comunicar sobre resultados de los an√°lisis.

La primera pregunta que me surgi√≥ fue ¬øQu√© es EDA? An√°lisis exploratorio de datos, es un proceso de investigaci√≥n en ciencia de datos que utiliza estad√≠sticas y visualizaciones para explorar conjunto de datos, descubrir patrones e indentificar problemas y generar nuevas preguntas.

En resumen, la idea es entender la estructura de los datos y preparar los mismos para todo tipo de an√°lisis y trabajo con estos datos.

En el caso de esta pr√°ctica, este EDA lo realizamos visual con la librer√≠a [Seaborn](https://seaborn.pydata.org/tutorial.html), esta funciona sobre Matplotlib que ya mencion√© anteriormente e integrando pandas para el trabajo con los datos.

---

### Preguntas finales, conclusiones.

**¬øQu√© variables parecen m√°s relacionadas con¬†`Survived`?**

Las variables m√°s relacionadas con qu√© persona sobrevive, seg√∫n el an√°lisis ser√≠an primero que nada la edad, donde a trav√©s de la gr√°fica de estas, podemos observar como se sigui√≥ la regla de ‚Äúni√±os y ancianos‚Äù primero, dado que son en estos dos polos donde se ve un mayor numero de supervivientes en funci√≥n de las cantidades totales de los mismos.

As√≠ mismo como segunda variable a resaltar es la del g√©nero, dado que podemos observar claramente que sobrevivieron mayormente mujeres, cabe mencionar que este numero tiene una implicancia en el entorno de edades que se encuentran entre los dos polos mencionados en el punto anterior, dado que luego de priorizar ancianos y ni√±os continuan las mujeres.

Como otro dato analizado y observado tenemos una clara implicancia de la clase socio-econ√≥mica de los pasajeros, donde tras investigar d√≥nde se encontraban los botes salvavidas en funcion de los costos de las distintas cabinas dado sentido en la cantidad de personas salvadas en funci√≥n de dicha clase.

Por √∫ltimo observamos la elevada correlaci√≥n entre el precio del ticket y las personas salvas, esto explicado sabiendo que los ni√±os y los ancianos son los que tuvieron un ticket m√°s accesible por dicho margen de edades.

**¬øD√≥nde hay m√°s valores faltantes? ¬øC√≥mo los imputar√≠as?**

Donde vemos datos no utilizados para el √°nalisis es en los que son de tipo String, como lo son: ‚Äúsibsp‚Äù, ‚Äúparch‚Äù y ‚Äúembarked‚Äù, donde los mismos para ser imputados para el √°nalisis requieren un procesamiento de datos previos, un pasaje a una escala n√∫merica para que puedan participar del √°nalisis matem√°tico, expresado en la tabla de correlaciones.

**¬øQu√© hip√≥tesis probar√≠as a continuaci√≥n?**

Como primera hip√≥tesis que se podr√≠a desarrollar es explayar la mencionada anteriormente de la relaci√≥n entre la ubicaci√≥n de los pasajeros en el barco en contraste con la ubicaci√≥n de los botes salvavidas.

---

## üì∏ Evidencias

[Enlace al notebook](https://colab.research.google.com/drive/1PjFidbLK2lcRPLRYjNs6gpr7ZHA-8Z_o?usp=sharing)

### üìä Salidas clave

- Setup Inicial:  
![setup inicial](../assets/ut1_p1_1.png){ width="480" }

- Instalaci√≥n de Kaggle:  
![kaggle_setup](../assets/ut1_p1_2.png){ width="480" }

- Vista previa del dataset:  
![Head del dataset](../assets/ut1_p1_3.png){ width="480" }

- Resumen del dataset (`shape`, `head`, `info`):  
![Resumen del dataset](../assets/ut1_p1_4.png){ width="480" }

- Estad√≠sticas descriptivas (`describe`):  
![Estad√≠sticas descriptivas](../assets/ut1_p1_5.png){ width="480" }

- Valores nulos ordenados:  
![Valores nulos](../assets/ut1_p1_6.png){ width="480" }

- Proporci√≥n de supervivientes:  
![Proporci√≥n de supervivientes](../assets/ut1_p1_7.png){ width="480" }

- C√≥digo para visualizaci√≥n:  
![C√≥digo visualizaci√≥n](../assets/ut1_p1_8.1.png){ width="480" }

- Salida de las gr√°ficas (supervivencia por sexo, clase, edad y correlaciones):  
![Visualizaciones EDA](../assets/ut1_p1_8.2.png){ width="480" }

---

## üí° Reflexi√≥n

Este primer pr√°ctico me permiti√≥ dar un paso real dentro del mundo del Machine Learning. No solo ejecut√© c√≥digo ya escrito, sino que me detuve a entender qu√© hac√≠a cada l√≠nea, qu√© herramientas usaba y por qu√© eran necesarias. Descubr√≠ la importancia de tener un entorno de trabajo organizado (Colab, Drive, Kaggle) y confirm√© que las librer√≠as como Pandas, Matplotlib y Seaborn son la base de cualquier an√°lisis de datos.

Creo que lo m√°s valioso fue comprender que un EDA no es solo mostrar gr√°ficos, sino aprender a leer los datos, detectar problemas y prepararlos para el siguiente nivel de an√°lisis. Me doy cuenta de que todav√≠a tengo que mejorar en automatizar mis flujos de trabajo y aprovechar m√°s la documentaci√≥n oficial en lugar de depender solo de ejemplos.

Como pr√≥ximos pasos, me gustar√≠a buscar la vuelta para una mejor organizaci√≥n para poder darme mejor el espacio para aprender las cosas, al menos conseguir ese nivel de aprendizaje o conocimiento donde al enfrentar problemas reales poder tener estas herramientas como una posibilidad para aplicar y encontrar soluciones.

---

## üìö Referencias

- [Google Colab Docs](https://colab.research.google.com/#scrollTo=vwnNlNIEwoZ8)
- [Kaggle Titanic Competition](https://www.kaggle.com/competitions/titanic/data)
- [Pandas API Reference](https://pandas.pydata.org/docs/reference/frame.html)
- [Matplotlib Docs](https://matplotlib.org/)
- [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html)