---
title: "ValidaciÃ³n y selecciÃ³n de modelos: pipelines, cross-validation y torneo de algoritmos"
date:
---

# ValidaciÃ³n y selecciÃ³n de modelos: pipelines, cross-validation y torneo de algoritmos

---

## ğŸ“ Contexto

En este quinto prÃ¡ctico de la UT1 el foco cambia por completo.  
Ya no estamos solamente entrenando modelos, sino aprendiendo a **validarlos correctamente**, compararlos de forma justa y evitar errores clÃ¡sicos como el **data leakage**.

Para esto trabajÃ© con el dataset *Student Dropout and Academic Success* del UCI Machine Learning Repository. Este dataset contiene informaciÃ³n demogrÃ¡fica, econÃ³mica y acadÃ©mica de estudiantes universitarios, y la tarea consiste en predecir si el estudiante:

- **0 â†’ Dropout**  
- **1 â†’ Enrolled**  
- **2 â†’ Graduate**

Este prÃ¡ctico fue uno de los mÃ¡s completos hasta ahora porque combinÃ³:

- Preprocesamiento seguro con *pipelines*  
- ValidaciÃ³n cruzada (KFold y StratifiedKFold)  
- ComparaciÃ³n sistemÃ¡tica de modelos (Logistic Regression, Ridge, Random Forest)  
- OptimizaciÃ³n de hiperparÃ¡metros con GridSearch y RandomizedSearch  
- Interpretabilidad mediante Feature Importance  

Como siempre, todas las salidas que aparecen en esta entrada fueron generadas en Google Colab.

---

## ğŸ¯ Objetivos

En esta prÃ¡ctica busquÃ©:

- Comprender cÃ³mo evitar **data leakage** utilizando pipelines.  
- Implementar **validaciÃ³n cruzada** para medir estabilidad de modelos.  
- Comparar distintos algoritmos de clasificaciÃ³n de manera justa y sistemÃ¡tica.  
- Seleccionar el mejor modelo considerando rendimiento **y variabilidad**.  
- Optimizar modelos mediante **GridSearchCV** y **RandomizedSearchCV**.  
- Analizar quÃ© caracterÃ­sticas son mÃ¡s importantes para explicar el resultado acadÃ©mico de un estudiante.

---

## ğŸš€ Desarrollo

### ğŸ“ Parte 1 â€“ Carga y ExploraciÃ³n del Dataset

Para comenzar, carguÃ© el dataset *Student Dropout and Academic Success* directamente desde el UCI ML Repository utilizando la librerÃ­a `ucimlrepo`. Este dataset contiene:

- **36 features** relacionadas con edad, notas, situaciÃ³n econÃ³mica, becas, historial acadÃ©mico, etc.  
- Una variable objetivo con tres posibles categorÃ­as:  
  - `Dropout`  
  - `Enrolled`  
  - `Graduate`

AdemÃ¡s de cargar los datos, analicÃ©:

- Cantidad total de estudiantes  
- DistribuciÃ³n de clases (muy desbalanceada hacia â€œGraduateâ€)  
- Primeras caracterÃ­sticas disponibles  
- EstadÃ­sticas bÃ¡sicas como la edad promedio de ingreso

A continuaciÃ³n, una captura de esta exploraciÃ³n:

![ExploraciÃ³n del dataset de estudiantes](../assets/ut1_p5_1.png){ width="480" }

---

### ğŸ”¬ Parte 2 â€“ ValidaciÃ³n Cruzada: midiendo estabilidad

Uno de los puntos centrales de esta prÃ¡ctica fue entender por quÃ© un simple *train/test split* no alcanza para evaluar modelos en datasets reales, sobre todo cuando hay **clases desbalanceadas** como en este caso.

Para eso implementÃ© dos tÃ©cnicas de validaciÃ³n cruzada:

#### 1ï¸âƒ£ **KFold tradicional**

Divide los datos en 5 partes y entrena/evalÃºa el modelo 5 veces, usando cada parte como â€œtestâ€ una vez.

#### 2ï¸âƒ£ **StratifiedKFold**

Hace lo mismo, pero manteniendo la **proporciÃ³n de clases** en cada fold.  
Esto es clave cuando una clase (por ejemplo *Graduate*) es muy dominante sobre las demÃ¡s.

Ambos mÃ©todos se aplicaron sobre un pipeline compuesto por:

- `StandardScaler()`  
- `LogisticRegression(max_iter=1000)`

Esto permite evaluar el modelo sin riesgo de data leakage porque el scaler se ajusta **solo con datos de cada fold**, no con todo el dataset.

---

#### ğŸ“Š Resultados de validaciÃ³n cruzada

En mi caso obtuve los siguientes resÃºmenes de accuracy:

- **KFold:** media â‰ˆ 0.49, desviaciÃ³n â‰ˆ 0.03  
- **StratifiedKFold:** media â‰ˆ 0.50, desviaciÃ³n â‰ˆ 0.02  

Lo mÃ¡s importante no es la media, sino la **estabilidad**: StratifiedKFold tuvo menor variabilidad, lo cual lo vuelve mÃ¡s confiable.

La siguiente imagen muestra las salidas completas junto al boxplot comparativo:

![Resultados KFold vs StratifiedKFold](../assets/ut1_p5_2.png){ width="480" }

---

#### ğŸ§  ConclusiÃ³n de esta parte

- **StratifiedKFold fue mÃ¡s estable**, por lo tanto es la tÃ©cnica recomendada para este dataset.  
- La diferencia de rendimiento es pequeÃ±a, pero la diferencia de *variabilidad* es clara.  
- Validar correctamente es tan importante como entrenar el modelo.

---

### ğŸ† Parte 3 â€“ Competencia de modelos: Â¿cuÃ¡l clasifica mejor?

DespuÃ©s de validar correctamente con StratifiedKFold, pasÃ© a comparar distintos modelos bajo las mismas condiciones. Para esto armÃ© un â€œtorneoâ€ donde cada modelo se evaluÃ³ con 5-Fold CV estratificado.

Los participantes fueron:

#### ğŸ”¹ **Logistic Regression**
Con estandarizaciÃ³n previa (`StandardScaler`).  
Es un modelo lineal, simple y rÃ¡pido.

#### ğŸ”¹ **Ridge Classifier**
Similar a la RegresiÃ³n LogÃ­stica, pero incorpora regularizaciÃ³n L2 que ayuda a controlar overfitting.

#### ğŸ”¹ **Random Forest**
Un ensamble de Ã¡rboles de decisiÃ³n.  
No requiere escalado y suele funcionar muy bien en datasets tabulares.

Cada modelo se evaluÃ³ en tÃ©rminos de:

- **Accuracy promedio**  
- **DesviaciÃ³n estÃ¡ndar (estabilidad)**  
- **DistribuciÃ³n de scores en cada fold**

---

#### ğŸ“ˆ Resultados globales del torneo

En mi ejecuciÃ³n, los valores aproximados fueron:

- **Random Forest:** 0.70 Â± 0.02  
- **Ridge Classifier:** 0.50 Â± 0.03  
- **Logistic Regression:** 0.50 Â± 0.02  

El ganador fue claramente:

#### ğŸ¥‡ **Random Forest**

Con mejor rendimiento promedio y con variabilidad baja. Este resultado tiene sentido porque:

- El dataset es grande  
- Tiene muchas features categÃ³ricas numÃ©ricas  
- Los Ã¡rboles capturan relaciones no lineales fÃ¡cilmente  

---

#### ğŸ“Š Visualizaciones del torneo

La siguiente imagen muestra los boxplots y barras comparativas:

![Competencia de modelos â€“ Boxplot y Barras](../assets/ut1_p5_3.png){ width="480" }

---

#### ğŸ§  ConclusiÃ³n de esta parte

El torneo deja claro que:

- No siempre el modelo lineal es suficiente  
- Los Ã¡rboles y ensambles suelen sobresalir en datos complejos  
- La estabilidad es tan importante como la media (un modelo inestable no sirve en producciÃ³n)

---

### âš™ï¸ Parte 4 â€“ OptimizaciÃ³n de hiperparÃ¡metros

Luego de identificar al ganador del torneo (Random Forest en mi caso), pasÃ© a la optimizaciÃ³n de hiperparÃ¡metros.  
La idea acÃ¡ es encontrar la mejor configuraciÃ³n del modelo usando bÃºsqueda sistemÃ¡tica.

TrabajÃ© con dos mÃ©todos:

#### 1ï¸âƒ£ **GridSearchCV**
Prueba *todas* las combinaciones posibles del espacio de bÃºsqueda.  
Es mÃ¡s lento, pero encuentra el Ã³ptimo global dentro del grid.

#### 2ï¸âƒ£ **RandomizedSearchCV**
Elige combinaciones aleatorias del grid.  
Es mÃ¡s rÃ¡pido y Ãºtil cuando el nÃºmero de combinaciones es muy grande.

---

#### ğŸ“‹ Espacio de bÃºsqueda utilizado

En mi ejecuciÃ³n, al ser el modelo ganador un **Random Forest**, se optimizaron parÃ¡metros como:

- `n_estimators` (cantidad de Ã¡rboles)  
- `max_depth` (profundidad mÃ¡xima)  
- `min_samples_split` (mÃ­nimo para dividir un nodo)

Ambos mÃ©todos fueron corridos con `cv=5` y `scoring="accuracy"`.

---

#### ğŸ“ˆ Resultados de la optimizaciÃ³n

El notebook devolviÃ³ resultados similares a:

- **Mejor score (GridSearch):** â‰ˆ 0.71  
- **Mejor score (RandomSearch):** â‰ˆ 0.70  
- El mejor set de hiperparÃ¡metros incluÃ­a una combinaciÃ³n equilibrada entre profundidad y cantidad de Ã¡rboles.

TambiÃ©n comparÃ© eficiencia:

- GridSearch probÃ³ *todas* las combinaciones  
- RandomSearch probÃ³ solo 20  
- El rendimiento final fue muy similar

---

#### ğŸ“Š VisualizaciÃ³n del proceso

La siguiente imagen muestra los resultados impresos en consola para ambos mÃ©todos:

![OptimizaciÃ³n con GridSearch y RandomizedSearch](../assets/ut1_p5_4.png){ width="480" }

---

#### ğŸ§  ConclusiÃ³n de esta parte

- GridSearch es ideal cuando el espacio es pequeÃ±o y queremos exhaustividad.  
- RandomSearch es mÃ¡s eficiente para espacios grandes.  
- En este dataset, ambos mÃ©todos convergieron a valores similares, lo que indica que el modelo no es extremadamente sensible a cambios finos en los hiperparÃ¡metros.

---

### ğŸ§© Parte 5 â€“ Interpretabilidad: entender por quÃ© el modelo decide lo que decide

Una vez elegido y optimizado el mejor modelo (Random Forest en mi caso), el siguiente paso fue analizar **cÃ³mo** toma decisiones. En aplicaciones reales, especialmente educativas o mÃ©dicas, la explicabilidad es tan importante como la precisiÃ³n.

En esta parte usÃ© tres enfoques:

---

#### 1ï¸âƒ£ Feature Importance: Â¿QuÃ© variables influyen mÃ¡s?

El Random Forest permite obtener la importancia relativa de cada caracterÃ­stica segÃºn cuÃ¡nto contribuye a reducir la impureza en los Ã¡rboles.

En mi ejecuciÃ³n, las variables mÃ¡s importantes fueron:

- Factores **acadÃ©micos** (notas, unidades aprobadas, desempeÃ±o por semestre)  
- Algunas variables **econÃ³micas**  
- Algunas **demogrÃ¡ficas** en menor medida

Estas importancias permiten entender *quÃ© patrones generales usa el modelo* para predecir abandono, continuidad o graduaciÃ³n.

---

#### 2ï¸âƒ£ Importancia por categorÃ­as

AgrupÃ© features en tres grandes grupos:

- AcadÃ©micos  
- DemogrÃ¡ficos  
- EconÃ³micos  

Esto permitiÃ³ entender quÃ© aspecto de la vida del estudiante pesa mÃ¡s.  
Los factores acadÃ©micos fueron la categorÃ­a con mayor contribuciÃ³n total, lo cual tiene sentido porque reflejan el rendimiento real del estudiante en el plan de estudios.

---

#### 3ï¸âƒ£ AnÃ¡lisis individual: explicaciÃ³n caso por caso

TambiÃ©n analicÃ© el caso de un estudiante especÃ­fico (*Ã­ndice 0*), revisando:

- La predicciÃ³n final (Dropout / Enrolled / Graduate)  
- Probabilidades por clase  
- Top 5 caracterÃ­sticas que mÃ¡s influyen en su clasificaciÃ³n  

Este tipo de anÃ¡lisis es clave para intervenciones:

- Si un estudiante estÃ¡ cerca del "Dropout", podemos ver **por quÃ©**  
- Y actuar sobre las caracterÃ­sticas de mayor impacto (por ejemplo, rendimiento bajo en ciertas materias)

---

#### 4ï¸âƒ£ VisualizaciÃ³n de Ã¡rboles individuales (resumen)

El Random Forest entrena muchos Ã¡rboles distintos.  
Para interpretabilidad, mostrÃ© algunos de ellos limitando su profundidad (`max_depth=3`) para hacerlos legibles.

Estos Ã¡rboles ayudan a ver reglas concretas del tipo:

> â€œSi la nota del segundo semestre < X y el nÃºmero de unidades aprobadas < Y, entonces mayor probabilidad de Dropout.â€

Este tipo de reglas permiten explicar decisiones con lÃ³gica comprensible.

---

#### ğŸ§  Â¿Por quÃ© importa la explicabilidad?

La explicabilidad no es opcional en contextos educativos:

- **Confianza:** Los docentes necesitan entender *por quÃ©* el modelo predice abandono.  
- **Intervenciones:** Saber quÃ© variables son importantes permite diseÃ±ar acciones especÃ­ficas.  
- **DetecciÃ³n de sesgos:** Permite identificar si el modelo favorece o penaliza grupos particulares.  
- **Regulaciones:** Muchos contextos requieren transparencia en modelos automatizados.  
- **Mejora continua:** Entender cÃ³mo decide el modelo permite mejorarlo en futuras versiones.

#### ğŸ“Š Importancia de caracterÃ­sticas

![Feature Importance â€“ Top 15](../assets/ut1_p5_5.png){ width="480" }

---

## ğŸ’¡ ReflexiÃ³n

Este prÃ¡ctico fue probablemente el mÃ¡s completo de toda la UT1 porque me obligÃ³ a pensar mÃ¡s allÃ¡ del â€œmodelo en sÃ­â€ y a enfocarme en **cÃ³mo validarlo y entenderlo**.

Por un lado, pude ver cÃ³mo una validaciÃ³n cruzada bien aplicada cambia por completo la percepciÃ³n del rendimiento del modelo.  
Antes pensaba en accuracy como un nÃºmero fijo; ahora entiendo que es mÃ¡s importante su **variabilidad** y si el modelo se mantiene estable frente a diferentes particiones de datos.

TambiÃ©n confirmÃ© lo peligroso que es el **data leakage** y cÃ³mo los pipelines ayudan a evitarlo automÃ¡ticamente. Algo tan simple como escalar los datos fuera del split puede inflar artificialmente los resultados sin que uno se dÃ© cuenta.

El torneo de modelos fue muy interesante porque mostrÃ³ que no siempre la RegresiÃ³n LogÃ­stica es suficiente. En este caso, el **Random Forest** no solo ganÃ³ en accuracy sino tambiÃ©n en estabilidad. Esto refuerza la idea de que en datasets grandes y con mÃºltiples variables, los Ã¡rboles y ensambles suelen capturar relaciones mÃ¡s complejas.

La parte de optimizaciÃ³n con GridSearchCV y RandomizedSearchCV me hizo ver cuÃ¡nto puede mejorar un modelo ajustando algunos hiperparÃ¡metros, y tambiÃ©n cuÃ¡ndo vale la pena invertir tiempo en una bÃºsqueda exhaustiva versus una bÃºsqueda aleatoria.

Finalmente, la secciÃ³n de **explicabilidad** fue la que mÃ¡s me aportÃ³ a nivel conceptual. Poder ver quÃ© caracterÃ­sticas pesan mÃ¡s para predecir abandono estudiantil da un valor prÃ¡ctico enorme. No es solo un nÃºmero: son insights accionables que ayudan a tomar decisiones reales en contextos educativos.

En resumen, esta prÃ¡ctica me ayudÃ³ a cerrar el cÃ­rculo completo:  
- preparar datos correctamente  
- validar de forma robusta  
- comparar modelos sin sesgos  
- optimizarlos  
- y explicar sus decisiones  

Siento que ahora tengo un entendimiento mucho mÃ¡s claro de cÃ³mo deberÃ­a encararse un problema real de Machine Learning en organizaciones.

---

## ğŸ“š Referencias

- [Dataset: Student Dropout and Academic Success â€“ UCI ML Repository](https://archive.ics.uci.edu/dataset/697/student+dropout+and+academic+success)
- [ucimlrepo â€“ Python library para cargar datasets del UCI Repository](https://pypi.org/project/ucimlrepo/)
- [DocumentaciÃ³n de LogisticRegression (Scikit-learn)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
- [DocumentaciÃ³n de RidgeClassifier (Scikit-learn)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html)
- [DocumentaciÃ³n de RandomForestClassifier (Scikit-learn)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
- [DocumentaciÃ³n de StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)
- [DocumentaciÃ³n de Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
- [DocumentaciÃ³n de KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)
- [DocumentaciÃ³n de StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)
- [DocumentaciÃ³n de cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)
- [DocumentaciÃ³n de GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
- [DocumentaciÃ³n de RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)
- [Feature Importance â€“ Ensembles de Ã¡rboles (User Guide)](https://scikit-learn.org/stable/modules/ensemble.html#feature-importance)
- [VisualizaciÃ³n de Ã¡rboles: plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)
- [RepresentaciÃ³n textual de Ã¡rboles: export_text](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html)
